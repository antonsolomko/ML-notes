{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computing gradients.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdmUrOCsGJ/Ga0OSzhMMKk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwwyKgi6o2JB",
        "colab_type": "text"
      },
      "source": [
        "# Computing Gradients\n",
        " \n",
        "In this note we briefly explain how to compute gradients of a loss function with respect to parameters.\n",
        " \n",
        "First recall some basic definitions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF0BTJHy2tin",
        "colab_type": "text"
      },
      "source": [
        "### Derivative and Gradient\n",
        " \n",
        "For a function $f\\colon\\mathbb{R}^n\\to\\mathbb{R}$,\n",
        "the **derivative** $\\frac{\\partial f}{\\partial x}$ is a *linear functional* (or a *covector*) $\\mathbb{R}^n\\to\\mathbb{R}$ that represents the \"linear part\" of the map: \n",
        "$$\n",
        "f(x+\\Delta x) = f(x) + \\frac{\\partial f}{\\partial x}\\Delta x + \\bar{o}(\\|\\Delta x\\|), \\quad \\Delta x\\in\\mathbb{R}^n,\n",
        "$$\n",
        "while the **gradient** $\\nabla_x f$ is a *vector* in $\\mathbb{R}^n$.\n",
        "If we represent $\\frac{\\partial f}{\\partial x}$ as a $1\\times n$ matrix, then $\\nabla_x f = \\left(\\frac{\\partial f}{\\partial x}\\right)^\\top$, both composed of *partial derivatives* $\\frac{\\partial f}{\\partial x_i}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N07y2xazM0sr",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent\n",
        " \n",
        "It follows that minimizing $f(x+\\Delta x)$, having $\\|\\Delta x\\|$ fixed, is equivalent to minimizing $\\frac{\\partial f}{\\partial x}\\Delta x$ and, by the Cauchy-Schwarz inequality, the minimum is achieved when $\\Delta x$ is proportional to $-\\nabla_x f$, which is thus the direction of the *steepest descent* of $f$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8niZv7hP3wWX",
        "colab_type": "text"
      },
      "source": [
        "### Jacobian\n",
        "\n",
        "More generally, if $f$ is a map $\\mathbb{R}^n \\to \\mathbb{R}^m$, then its **derivative** is a *linear map* $\\frac{\\partial f}{\\partial x} \\colon \\mathbb{R}^n \\to \\mathbb{R}^m$ such that\n",
        "$$\n",
        "f(x+\\Delta x) = f(x) + \\frac{\\partial f}{\\partial x}\\Delta x + \\bar{o}(\\|\\Delta x\\|), \\quad \\Delta x\\in\\mathbb{R}^n.\n",
        "$$\n",
        "In coordinate form it is represented by the $m\\times n$ **Jacobian matrix** $\\frac{\\partial f}{\\partial x} = \\left(\\frac{\\partial f_i}{\\partial x_j}\\right)_{ij}$ composed of *partial derivatives*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqJyI5x88jAQ",
        "colab_type": "text"
      },
      "source": [
        "### Chain Rule\n",
        " \n",
        "Suppose now that we have two maps $\\mathbb{R}^n \\overset{f}{\\to} \\mathbb{R}^m \\overset{g}{\\to} \\mathbb{R}$.\n",
        "If $y = f(x)$ and $z=g(y)$, then, by the **chain rule**, $$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$$ and hence \n",
        "$$\n",
        "\\nabla_x z = \\left(\\frac{\\partial y}{\\partial x}\\right)^\\top \\nabla_y z\n",
        ".$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wVrg5TRIEty",
        "colab_type": "text"
      },
      "source": [
        "### Dimensions\n",
        " \n",
        "By convention, above we represented vectors $x\\in\\mathbb{R}^n$ and gradients $\\nabla_x f$ as *column vectors*, i.e. $n\\times 1$ matrices.\n",
        "If, however, $x$ has different shape, then it is convenient to reshape $\\nabla_x f$ accordingly to match the dimensions.\n",
        "So $\\nabla_x f$ can be a *tensor* of any dimension, if so is $x$: each element of $\\nabla_x f$ is the partial derivative of $f$ w.r.t. the corresponding element of $x$.\n",
        "\n",
        "If $f\\colon\\mathbb{R}\\to\\mathbb{R}$ is a *scalar function* and $x$ is a tensor, then $f(x)$ means applying $f$ elementwise (*broadcasting*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfjH6i0xK81D",
        "colab_type": "text"
      },
      "source": [
        "## Examples\n",
        "\n",
        "Let's start with a warm up and compute some derivatives that will be useful later.\n",
        "\n",
        "1. If $y = Ax$, where $A\\in\\mathbb{R}^{m\\times n}$ and $x\\in\\mathbb{R}^{n\\times 1}$, then the Jacobian $\\frac{\\partial y}{\\partial x} = A$, because\n",
        "$y_i = \\sum_k a_{ik} x_k$ and $\\frac{\\partial y_i}{\\partial x_j} = a_{ij}$.\n",
        "\n",
        "2. If, on the other hand, $y = xA$, with $x\\in\\mathbb{R}^{1\\times n}$ and $A\\in\\mathbb{R}^{n\\times m}$, then $\\frac{\\partial y}{\\partial x} = A^\\top$, because in this case\n",
        "$y_i = \\sum_k x_k a_{ki}$ and $\\frac{\\partial y_i}{\\partial x_j} = a_{ji}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAFvSf7pcdXa",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression\n",
        "\n",
        "Given\n",
        "\n",
        "* the *traning set* $X\\in\\mathbb{R}^{n\\times m}$ and $y\\in\\mathbb{R}^{1\\times m}$,\n",
        " \n",
        "* the *parameters* $w\\in\\mathbb{R}^{n\\times 1}$ and $b\\in\\mathbb{R}$,\n",
        " \n",
        "* $h = w^\\top X + b$ and $J = \\frac{1}{2m}\\|h - y\\|^2$ the *cost function*,\n",
        " \n",
        "we want to compute the gradients of the cost function $J$ with respect to the parameters $w$ and $b$.\n",
        "In order to apply the chain rule\n",
        "$$\n",
        "\\nabla_w J =\n",
        "\\left(\\frac{\\partial h}{\\partial w}\\right)^\\top \\nabla_h J\n",
        "$$\n",
        "we need the Jacobian $\\frac{\\partial h}{\\partial w}$ and the gradient $\\nabla_h J$:\n",
        "* $\\frac{\\partial h}{\\partial w} = X^\\top$, as we saw already, and\n",
        "* $\\nabla_h J = \\frac{1}{m}(h-y)^\\top$, because $\\frac{\\partial J}{\\partial h_i} = \\frac{1}{2m} \\frac{\\partial}{\\partial h_i} \\sum_{j=1}^m (h_j - y_j)^2 = \\frac{1}{m}(h_i-y_i)$.\n",
        "\n",
        "Therefore,\n",
        "$$\n",
        "\\nabla_w J = \\frac{1}{m} X(h-y)^\\top.\n",
        "$$\n",
        " \n",
        "Similarly, $\\frac{\\partial h}{\\partial b} = \\bar 1 \\in \\mathbb{R}^{1\\times m}$ and \n",
        "$$\n",
        "\\nabla_b J = \\frac{1}{m} \\sum_{i=1}^m (h_i-y_i).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAEV7JZEn8WT",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "* $X\\in\\mathbb{R}^{n\\times m}$ and $y\\in\\mathbb{R}^{1\\times m}$ is the *traning set*.\n",
        " \n",
        "* $w\\in\\mathbb{R}^{n\\times 1}$ and $b\\in\\mathbb{R}$ are the *parameters* that we want to train.\n",
        " \n",
        "* $z = w^\\top X + b$, $a = \\sigma(z)$ and $J = -\\frac 1m\\sum_{i=1}^m \\left(y_i\\log a_i + (1-y_i)\\log(1-a_i)\\right)$ is the *cost function*,\n",
        "\n",
        "where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the *sigmoid function*, $\\frac{d}{dz} \\sigma = \\sigma(z)(1-\\sigma(z)) = a(1-a)$.\n",
        "\n",
        "As before, we need to know $\\nabla_z J$. By the ordinary chain rule,\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial z_i} \n",
        "= \\frac{da_i}{dz_i} \\frac{\\partial J}{\\partial a_i}\n",
        "= -\\frac {a_i(1-a_i)}m \\left( \\frac{y_i}{a_i} - \\frac{1-y_i}{1-a_i} \\right) = \\frac{a_i-y_i}m,\n",
        "$$\n",
        "and we get $\\nabla_z J = \\frac 1m (a-y)^\\top$.\n",
        "Hence\n",
        "$$\n",
        "\\nabla_w J \n",
        "= \\left(\\frac{\\partial z}{\\partial w}\\right)^\\top \\nabla_z J\n",
        "= \\frac 1m X (a-y)^\\top,\n",
        "$$\n",
        "$$\n",
        "\\nabla_b J \n",
        "= \\left(\\frac{\\partial z}{\\partial b}\\right)^\\top \\nabla_z J\n",
        "= \\frac{1}{m} \\sum_{i=1}^m (a_i-y_i).\n",
        "$$"
      ]
    }
  ]
}